### Day 24: Generative Adversarial Networks (GANs)

#### Understanding the Basics of GANs

Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. GANs consist of two neural networks, the generator and the discriminator, which compete against each other in a zero-sum game framework.

1. **Generator**: The generator takes random noise as input and generates data that resembles the training data. Its goal is to produce data that is indistinguishable from real data.

2. **Discriminator**: The discriminator receives both real data and fake data generated by the generator. Its goal is to correctly distinguish between real and fake data.

The training process involves the generator trying to fool the discriminator, while the discriminator tries to correctly identify real versus fake data. This adversarial process continues until the generator produces data that is indistinguishable from the real data to the discriminator.

#### Key Concepts

- **Adversarial Loss**: The loss function used to train GANs, which typically includes a component for the discriminator and a component for the generator.
- **Minimax Game**: The training process can be viewed as a minimax game where the generator tries to minimize the loss while the discriminator tries to maximize it.
- **Latent Space**: The input to the generator, usually a vector of random noise, which is transformed into data resembling the training data.

#### Building and Training a Simple GAN for Image Generation

Let's build a simple GAN to generate images using the MNIST dataset (handwritten digits). We'll use TensorFlow and Keras for this implementation.

1. **Import Libraries**

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
```

2. **Load and Preprocess Data**

```python
# Load the MNIST dataset
(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()

# Normalize the images to the range [-1, 1]
X_train = (X_train - 127.5) / 127.5
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
```

3. **Define the Generator**

```python
def build_generator():
    model = Sequential()
    model.add(Dense(256, input_dim=100))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(1024))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(28 * 28 * 1, activation='tanh'))
    model.add(Reshape((28, 28, 1)))
    return model
```

4. **Define the Discriminator**

```python
def build_discriminator():
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28, 1)))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(256))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    return model
```

5. **Compile the Models**

```python
# Build and compile the discriminator
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])

# Build the generator
generator = build_generator()

# Create the GAN by stacking the generator and discriminator
z = tf.keras.Input(shape=(100,))
img = generator(z)
discriminator.trainable = False
valid = discriminator(img)

gan = tf.keras.Model(z, valid)
gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
```

6. **Train the GAN**

```python
def train(epochs, batch_size=128, save_interval=50):
    # Load and preprocess the data
    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
    X_train = (X_train - 127.5) / 127.5
    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)

    # Labels for real and fake data
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    for epoch in range(epochs):
        # Train the discriminator
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_imgs = X_train[idx]

        noise = np.random.normal(0, 1, (batch_size, 100))
        gen_imgs = generator.predict(noise)

        d_loss_real = discriminator.train_on_batch(real_imgs, valid)
        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator
        noise = np.random.normal(0, 1, (batch_size, 100))
        g_loss = gan.train_on_batch(noise, valid)

        # Print the progress
        print(f"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]")

        # Save generated images at save intervals
        if epoch % save_interval == 0:
            save_imgs(epoch)

def save_imgs(epoch):
    r, c = 5, 5
    noise = np.random.normal(0, 1, (r * c, 100))
    gen_imgs = generator.predict(noise)

    # Rescale images 0 - 1
    gen_imgs = 0.5 * gen_imgs + 0.5

    fig, axs = plt.subplots(r, c)
    cnt = 0
    for i in range(r):
        for j in range(c):
            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')
            axs[i, j].axis('off')
            cnt += 1
    fig.savefig(f"images/mnist_{epoch}.png")
    plt.close()

# Train the GAN
train(epochs=10000, batch_size=64, save_interval=200)
```

This code will train a simple GAN on the MNIST dataset and save generated images at regular intervals. The generator and discriminator are both simple feedforward neural networks. The generator takes a 100-dimensional noise vector as input and produces a 28x28 image, while the discriminator takes a 28x28 image as input and outputs a probability that the image is real.

#### Summary

Today, you learned the basics of GANs and built a simple GAN for image generation using the MNIST dataset. You now have a foundational understanding of how GANs work and how to implement them using TensorFlow and Keras.
